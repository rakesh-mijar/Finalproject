
import os
from PIL import Image
import matplotlib.pyplot as plt

# Define your dataset directory
dataset_dir = "/content/drive/MyDrive/Final_Project/archive (3)/Original"

# Function to display one image from each subfolder
def display_one_image_per_subfolder(dataset_dir):
    for subdir in os.listdir(dataset_dir):
        subdir_path = os.path.join(dataset_dir, subdir)

        if os.path.isdir(subdir_path):
            # Get the list of image files in the subfolder
            image_files = [f for f in os.listdir(subdir_path) if f.endswith(('.jpg', '.jpeg', '.png'))]

            if image_files:
                # Take the first image from the list
                first_image_path = os.path.join(subdir_path, image_files[0])

                # Load and display the image
                img = Image.open(first_image_path)
                plt.imshow(img)
                plt.title(f"Image from {subdir}")
                plt.show()

# Call the function to display one image from each subfolder
display_one_image_per_subfolder(dataset_dir)
----------------------------------------------------------------------------------------------------

#checking the size of images previous
from PIL import Image
import os

# Define your dataset directory
dataset_dir = "/content/drive/MyDrive/Final_Project/archive (3)/Original"

# Function to get the size of all images in a directory
def get_image_sizes(directory):
    sizes = []

    for subdir in os.listdir(directory):
        subdir_path = os.path.join(directory, subdir)

        if os.path.isdir(subdir_path):
            for filename in os.listdir(subdir_path):
                if filename.endswith(('.jpg', '.jpeg', '.png')):
                    # Load the image
                    img_path = os.path.join(subdir_path, filename)
                    img = Image.open(img_path)

                    # Get the size of the image
                    img_size = img.size
                    sizes.append((img_path, img_size))

    return sizes

# Call the function to get image sizes in the dataset directory
image_sizes = get_image_sizes(dataset_dir)

# Print the image sizes
for img_path, img_size in image_sizes:
    print(f"Image: {img_path}, Size: {img_size}")

------------------------------------------------------------------------------------

#image resizing
from PIL import Image
import os

# Define your dataset directory
dataset_dir = "/content/drive/MyDrive/Final_Project/archive (3)/Original"

# Define the target size for resizing
target_size = (224, 224)  # Adjust as needed

# Function to resize all images in a directory
def resize_images_in_directory(directory, target_size):
    for subdir in os.listdir(directory):
        subdir_path = os.path.join(directory, subdir)

        if os.path.isdir(subdir_path):
            for filename in os.listdir(subdir_path):
                if filename.endswith(('.jpg', '.jpeg', '.png')):
                    # Load the image
                    img_path = os.path.join(subdir_path, filename)
                    img = Image.open(img_path)

                    # Resize the image
                    resized_img = img.resize(target_size)

                    # Save the resized image (overwrite the original or save to a new directory)
                    resized_img.save(img_path)

# Call the function to resize images in the dataset directory
resize_images_in_directory(dataset_dir, target_size)
--------------------------------------------------------------------------------------------------------------
import os
import shutil
from sklearn.model_selection import train_test_split

# Define your main dataset directory
main_dataset_dir = "/content/drive/MyDrive/Final_Project/archive (3)/Original"

# Define the directories for benign and pro images
benign_dir = os.path.join(main_dataset_dir, "Benign")
pro_dir = os.path.join(main_dataset_dir, "Pro")

# Define the directories for training and test sets
train_dir = "/content/drive/MyDrive/Final_Project/archive (3)/Original/Train1"
test_dir = "/content/drive/MyDrive/Final_Project/archive (3)/Original/Test1"

# Create the training and test directories
os.makedirs(os.path.join(train_dir, "Benign"), exist_ok=True)
os.makedirs(os.path.join(train_dir, "Pro"), exist_ok=True)
os.makedirs(os.path.join(test_dir, "Benign"), exist_ok=True)
os.makedirs(os.path.join(test_dir, "Pro"), exist_ok=True)

# Function to split the dataset into training and test sets
def split_dataset(src_dir, train_dst, test_dst, test_size=0.2, random_state=42):
    # Get the list of all image files in the source directory
    all_images = [f for f in os.listdir(src_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]

    # Split the images into training and test sets
    train_images, test_images = train_test_split(all_images, test_size=test_size, random_state=random_state)

    # Copy images to the training set
    for img in train_images:
        src_path = os.path.join(src_dir, img)
        dst_path = os.path.join(train_dst, img)
        shutil.copy(src_path, dst_path)

    # Copy images to the test set
    for img in test_images:
        src_path = os.path.join(src_dir, img)
        dst_path = os.path.join(test_dst, img)
        shutil.copy(src_path, dst_path)

# Split the dataset for benign images
split_dataset(benign_dir, os.path.join(train_dir, "Benign"), os.path.join(test_dir, "Benign"))

# Split the dataset for pro images
split_dataset(pro_dir, os.path.join(train_dir, "Pro"), os.path.join(test_dir, "Pro"))
-------------------------------------------------------------------------------------------------------------

def count_images_in_directory(directory):
    return sum([len(files) for _, _, files in os.walk(directory)])

# Specify the paths to your train and test directories
train_dir = "/content/drive/MyDrive/Final_Project/archive (3)/Original/Train1"
test_dir = "/content/drive/MyDrive/Final_Project/archive (3)/Original/Test1"

# Count the number of images in the train and test directories
num_train_images = count_images_in_directory(train_dir)
num_test_images = count_images_in_directory(test_dir)

# Print the results
print(f"Number of images in the train dataset: {num_train_images}")
print(f"Number of images in the test dataset: {num_test_images}")
----------------------------------------------------------------------------------------------
#Data augmenation
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
import os

# Define the path to your original training dataset
original_train_dir = "/content/drive/MyDrive/Final_Project/archive (3)/Original/Train1"

# Define the path to your augmented training dataset
augmented_train_dir = "/content/drive/MyDrive/Final_Project/archive (3)/Original/Augmented1"

# Create an ImageDataGenerator with augmentation parameters
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip=True,
    fill_mode="nearest"
)

# Iterate through each class (benign and pro)
for class_name in os.listdir(original_train_dir):
    class_path = os.path.join(original_train_dir, class_name)

    # Create separate folders for augmented images
    augmented_class_path = os.path.join(augmented_train_dir, class_name)
    os.makedirs(augmented_class_path, exist_ok=True)

    # Iterate through images in the original class folder
    for filename in os.listdir(class_path):
        img_path = os.path.join(class_path, filename)
        img = load_img(img_path)
        x = img_to_array(img)
        x = x.reshape((1,) + x.shape)

        # Generate augmented images and save to the new folder
        i = 0
        for batch in datagen.flow(x, batch_size=1, save_to_dir=augmented_class_path, save_prefix='aug', save_format='jpeg'):
            i += 1
            if i >= 5:  # Adjust the number of augmented images to generate
                break
-------------------------------------------------------------------------------------------------------------------------------
import os
def count_images_in_directory(directory):
    return sum([len(files) for _, _, files in os.walk(directory)])

# Specify the paths to your train and test directories
train_dir = "/content/drive/MyDrive/Final_Project/archive (3)/Original/Augmented1"
test_dir = "/content/drive/MyDrive/Final_Project/archive (3)/Original/Test1"

# Count the number of images in the train and test directories
num_train_images = count_images_in_directory(train_dir)
num_test_images = count_images_in_directory(test_dir)

# Print the results
print(f"Number of images in the train dataset: {num_train_images}")
print(f"Number of images in the test dataset: {num_test_images}")
--------------------------------------------------------------------------------------------------------------------------------
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import Xception
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Define the path to your augmented and resized dataset
train_dataset_path = '/content/drive/MyDrive/Final_Project/archive (3)/Original/Augmented1'
test_dataset_path = '/content/drive/MyDrive/Final_Project/archive (3)/Original/Test1'

# Define image size and batch size
image_size = (224, 224)  # Adjust based on the input size expected by Xception
batch_size = 32

# Create an ImageDataGenerator for data normalization (no need for rescaling)
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip=True,
)

# Generate training dataset
train_generator = datagen.flow_from_directory(
    train_dataset_path,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='binary'
)

# Generate test dataset
test_generator = datagen.flow_from_directory(
    test_dataset_path,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='binary'
)

# Build Xception model with a custom dense layer for binary classification
base_model = Xception(weights='imagenet', include_top=False, input_shape=(image_size[0], image_size[1], 3))
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(128, activation='relu')(x)
predictions = Dense(1, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=predictions)

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Compile the model
model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
epochs = 20  # Adjust the number of epochs as needed
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    validation_data=test_generator,
    validation_steps=test_generator.samples // batch_size,
    epochs=epochs
)

# Save the trained model
model.save('/content/drive/MyDrive/Final_Project/archive (3)/Original/xception_model.h5')

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

# Predictions on the test set
predictions = model.predict(test_generator)
predicted_labels = (predictions > 0.5).astype(int)

# True labels
true_labels = test_generator.classes

# Confusion Matrix
conf_matrix = confusion_matrix(true_labels, predicted_labels)
print('Confusion Matrix:')
print(conf_matrix)

# Classification Report
print('Classification Report:')
class_report = classification_report(true_labels, predicted_labels)
print(class_report)

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()
